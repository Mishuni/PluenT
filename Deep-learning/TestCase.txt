	Hello, I'm Jung Sung Won. I'd like to talk about the software aspect of self driving cars. As we talked first, perception should precede the actions of self-driving cars to predict, control, and so on. In order for self driving cars to recognize the world, they need to be able to identify the images they received through their hardware, such as radar or liarrs. You need the ability to locate the objects placed in the images and to categorize them.
	While human beings easily categorize and recognize objects encountered while driving into traffic signs, people, and cars, computer recognition requires more complex tasks. To solve this problem in the 1950s, Frank Rosenblat devised Perceptron in the sense of perception neurons. Perceptron is the most basic model of the artificial neural network, consisting of two layers of input and output. Learn the weights between neurons belonging to each layer and apply the active function of linear combinations of N inputs to produce probability values between 0 and 1. 
	However, there was a fundamental limitation in that the weight setting should be done directly by humans and that the distribution of data in the form of XOR, such as on the right, could not be classified. 
	In 1985, Jeffrey Hinton developed Multi-layer Perceptron (MLP). Unlike the existing Single-layer perceptron, MLP solves the XOR problem that the existing single-layer perceptron could not solve by adding one or more intermediate layers between the input layer and the output layer.
 	However, the MLP also had a fundamental limitation. There were too many parameters to be trained first. For example, if you recognize handwriting with 16*16-sized font, the network will require a total of 28,326 weights and biases when MLP is configured with 256 input channels, 100 hidden-layer, and 26 output channels for handwriting recognition.This amount of parameters is required to recognize the alphabet in a simple neural network configuration, indicating that larger font sizes, more than two hidden layers, or an increase in the number of required parameters will result in an increase in the number of required parameters, which will make learning difficult.
	In addition, MLP should process with new learning data even if the image shows only a slight detachment, as shown above. As a result, there is a lot of learning data required and a corresponding learning time payment is required.
	In other words, it was difficult to expect computers to perform at a human level due to fundamental problems such as training time, number of parameters, and network size. 
	The problems with these MLPs arise because they believe that all inputs from the MLP have the same level of importance regardless of their location. But in practice, when humans process visual information, external stimuli only affect some of the human receptors, and the influence varies with the location of the stimuli. Convolution natural networks (CNNs) solve problems in existing MLPs by applying location-specific correlations as humans process visual information, and are taking the lead in recent computer vision.
	So let's take a look at the structure of CNN. The above figure is a typical CNN architecture. Unlike conventional MLPs, CNN deploys neurons in three dimensions in width, length, and depth and connects only to a portion of the front layer, not to the entire neuron. Each layer then enters a 3D volume and outputs a 3D volume through a differentiateable function. 
	CNN is mainly composed of synthetic layer, pooling layer, and fully-connected layer. 
	First of all, the synthetic layer is a key component of CNN. The parameters of the convolution layer are made up of a series of learnable filters, each small but deep in a horizontal dimension. Convolve the input volume of each filter in a horizontal or vertical way and pass forward by printing a two-dimensional acceleration map. When you slide the filter over the input, an internal operation is made between the filter and the elements of the input as shown below. Like this.
	This neural network learns filters that respond to a particular pattern at a particular location of an input, which builds these activation maps along a depth dimension into the output volume. Thus, each element of the output volume deals with only a small area of the input, and neurons in the same activation map share the same parameters as a result of applying the same filter.
	CNN shows a big difference from existing full connected MLPs in the use of con-volutive layers and recertive fields, where each neuron in the layer is connected only to the local zone of the input volume. For example, suppose you have an input image with an input volume size of 32*32*3 (low*length* depth). If the size of the recertive field is 5*5, each neuron in the constriction layer will apply light to an area of 5*5*3 size in the input image, using a total of 5*5*3 weights. For MLPs where all neurons are connected, it is much more efficient compared to requiring approximately 3 million parameters for input image processing of the same size. These features are called local connectivity and are typical of CNN.
	The pooling layer is then a layer that exists between successive convolutioan layers, and plays a role of reducing the spacial size of the presentation to reduce the number of parameters or the amount of computations in the network. The technique is used in the assumption that only best match selection with spacial sub-sampling characteristics is used because there are more overlapping parts adjacent. The most commonly used pooling technique is max pooling, which selects only the largest number from one subregion. The figure below shows an example of max pooling, which can reduce the calculation cost by 75% when 2*2 filter, 2 slide.  As a result of using only the best match selection, it can also be robusted in the damage detection to prevent overfitting. However, the pooling layer is gradually disabled when the dataset is large because it greatly reduces the size of the presentation.
	Finally, fully connected layer is CNN's last layer and is linked to all the activations of previous layers. Therefore, the activation of the fully connected layer can be obtained by multiplying the matrix by adding bias. The performance of CNN can be improved by changing the number of filters, the shape of the filter, strain value, zero-padding support, layer size, etc. in the above basicarchitecture.
Let's take a look at the performance of this CNN model. Usually, the results of the ILSVRC competition are mainly used to measure the performance of a computer vision. The ILSVRC is an ordinary computer version development developed by a subset of a publicly available computer version called called called ImageNet.mepage, there are a little more than 14 million images in the dataset, a little more than 21 groups or classes, and a little more than 1 million images that have bounding box options. ImageNet database
	Let's take a look at the competition statistics, which is a graph of the recognition error rates of the ILSVRC winners. At the start of the 2010 competition, the error rate was 28.2%, but since the advent of CNN based-winner for the first time in 2012, the video recognition error rate has dropped dramatically, and you can see that ResNet surpassed the human error rate for the first time in 2015. So from a software perspective, when deep learning is used, self-driving car is enough to carry out human awareness without human intervention.
	In our previous presentation, we looked at how self-driving car can recognize the world without human intervention in terms of hardware and software. In conclusion, Radar and CNN have shown that we can do more than human cognitive performance without human intervention. Thank you.

